{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7c535c",
   "metadata": {},
   "source": [
    "# Find the treasure game with Q Learning\n",
    "\n",
    "Aim of this game was for me to learn Q learning basically. Game is about finding the bigger treasure lying at some point in the one dimensional finite map. \n",
    "\n",
    "Game includes:\n",
    "\n",
    "-Two treasures (positive reward), one of them is bigger than the other.\n",
    "\n",
    "-If agent drops out of the map it gets punishment (negative reward) and the game finishes.\n",
    "\n",
    "-Main goal is for agent to learn its way to the big treasure as fast as possible and maximize the amount of reward it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2bedf05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [-0.01        0.          0.        ]\n",
      " [ 0.          0.          0.2821435 ]\n",
      " [ 0.24706205  0.28242856  0.31381059]\n",
      " [ 0.2824293   0.31377864  0.34867844]\n",
      " [ 0.3138106   0.34867844  0.38742049]\n",
      " [ 0.34867844  0.38742049  0.43046721]\n",
      " [ 0.38742049  0.43046721  0.4782969 ]\n",
      " [ 0.43046721  0.4782969   0.531441  ]\n",
      " [ 0.4782969   0.531441    0.59049   ]\n",
      " [ 0.531441    0.59049     0.6561    ]\n",
      " [ 0.59049     0.6561      0.729     ]\n",
      " [ 0.6561      0.729       0.81      ]\n",
      " [ 0.729       0.81        0.9       ]\n",
      " [ 0.81        0.9         1.        ]\n",
      " [ 0.9         1.          0.9       ]\n",
      " [ 1.          0.9         0.81      ]\n",
      " [ 0.9         0.81        0.729     ]\n",
      " [ 0.81        0.72899999  0.61677155]\n",
      " [ 0.72697732  0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the Q-table\n",
    "#Every row in the Q table corresponds to a positin on which agent be\n",
    "#Every columns corresponds to each action\n",
    "Qtable = np.zeros((21,3)) #For this state, map consists of 21 discrete positions and there are 3 actions that agent take\n",
    "\n",
    "#Action space, agent can go backwards, forwards or choose the say in the same position \n",
    "actions = np.array([-1,0,1])\n",
    "\n",
    "#Value for epsilon-greedy policy\n",
    "epsilon = 0.5\n",
    "\n",
    "#Learning rates\n",
    "eta = 0.9\n",
    "gamma = 0.9\n",
    "\n",
    "#Number of games\n",
    "num_games = 100\n",
    "\n",
    "#Number of steps per game\n",
    "num_steps = 100\n",
    "\n",
    "#Positions of the treasure\n",
    "treasure_1 = 10\n",
    "treasure_2 = 15\n",
    "\n",
    "#Amount of the treasures\n",
    "reward_1 = 10\n",
    "reward_2 = 100\n",
    "\n",
    "#Initial position\n",
    "start = 5\n",
    "\n",
    "for i in range(num_games):\n",
    "    \n",
    "    position = start\n",
    "\n",
    "    x = np.array([position])\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for j in range(num_steps):\n",
    "\n",
    "        if random() < epsilon:\n",
    "\n",
    "            next_action = np.random.choice(actions)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            if Qtable[position][0] == 0 and Qtable[position][1] == 0 and Qtable[position][2] == 0:\n",
    "                \n",
    "                next_action = np.random.choice(actions)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                next_action = actions[np.argmax(Qtable[position])]\n",
    "\n",
    "        new_position = position + next_action\n",
    "        \n",
    "        reward = 0\n",
    "         \n",
    "        if new_position == treasure_1:\n",
    "            reward = 10\n",
    "            total_reward += reward_1\n",
    "            \n",
    "        elif new_position == treasure_2:\n",
    "            reward = 100\n",
    "            total_reward += reward_2\n",
    "            \n",
    "        elif new_position == 0:\n",
    "            \n",
    "            #Punishment\n",
    "            reward = -10\n",
    "            \n",
    "            #Bellman equations for updating Q-table\n",
    "            Qtable[position, next_action + 1] = Qtable[position, next_action + 1] + eta*(reward + gamma*(np.max(Qtable[new_position])) - Qtable[position,next_action + 1])\n",
    "            position = new_position\n",
    "            x = np.append(x,position)\n",
    "            break\n",
    "            \n",
    "        elif new_position == 20:\n",
    "            \n",
    "            reward = -10\n",
    "            Qtable[position, next_action + 1] = Qtable[position, next_action + 1] + eta*(reward + gamma*(np.max(Qtable[new_position])) - Qtable[position,next_action + 1])\n",
    "            position = new_position\n",
    "            x = np.append(x,position)\n",
    "            break\n",
    "            \n",
    "        Qtable[position, next_action + 1] = Qtable[position, next_action + 1] + eta*(reward + gamma*(np.max(Qtable[new_position])) - Qtable[position,next_action + 1])\n",
    "        position = new_position\n",
    "        x = np.append(x,position)\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.plot(x)\n",
    "    \n",
    "print(Qtable/np.max(Qtable))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
